{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from helper import get_train_test, lowercase, tokenize, remove_punctuation, remove_stopwords, \\\n",
    "    remove_non_letters, stemming, correct_spelling, reduce_lengthening, subsampling, save_preprocessed_data, \\\n",
    "    get_categories, get_train_test_data, get_label_encoder\n",
    "    \n",
    "config = {\n",
    "    'lowercase': True,\n",
    "    'stemming': True,\n",
    "    'remove_stopwords': True,\n",
    "    'remove_non_letters': True,\n",
    "    'remove_punctuation': True,\n",
    "    'correct_spelling': True,\n",
    "    'reduce_lengthening': True,\n",
    "    'subsampling': False,\n",
    "    'use_checkpoint':True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['use_checkpoint']:\n",
    "    train, test = get_train_test_data('train_data.txt', 'test_data.txt')\n",
    "\n",
    "    # subsampling\n",
    "    if config['subsampling']:\n",
    "        subsampling(train)\n",
    "        subsampling(test)\n",
    "\n",
    "    # lowercasing\n",
    "    if config['lowercase']:\n",
    "        lowercase(train)\n",
    "        lowercase(test)\n",
    "\n",
    "    # removing non letters\n",
    "    if config['remove_non_letters']:\n",
    "        remove_non_letters(train)\n",
    "        remove_non_letters(test)\n",
    "\n",
    "    # removing punctuation\n",
    "    if config['remove_punctuation']:\n",
    "        remove_punctuation(train)\n",
    "        remove_punctuation(test)\n",
    "\n",
    "    # tokenizing\n",
    "    tokenize(train)\n",
    "    tokenize(test)\n",
    "\n",
    "    # reducing length\n",
    "    if config['reduce_lengthening']:\n",
    "        reduce_lengthening(train)\n",
    "        reduce_lengthening(test)\n",
    "\n",
    "    # correcting spelling\n",
    "    if config['correct_spelling']:\n",
    "        correct_spelling(train)\n",
    "        correct_spelling(test)\n",
    "\n",
    "    # removing stopwords\n",
    "    if config['remove_stopwords']:\n",
    "        remove_stopwords(train)\n",
    "        remove_stopwords(test)\n",
    "\n",
    "    # stemming\n",
    "    if config['stemming']:\n",
    "        stemming(train)\n",
    "        stemming(test)\n",
    "\n",
    "    # saving preprocessed data\n",
    "    save_preprocessed_data(train, 'train_data.pkl')\n",
    "    save_preprocessed_data(test, 'test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_preprocessed_data\n",
    "\n",
    "train, test = load_preprocessed_data('train_data.pkl', 'test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a Categories frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = pd.value_counts(train['category'])\n",
    "pd.DataFrame(category_counts,columns=['Counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts.sort_values(ascending=False).plot(kind='bar',figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b Avarage sentence length per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence_length'] = train_df.apply(lambda x: len(x['content']) + len(x['subject']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(train_df.category).mean().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c percentage of empty-content samples per catgeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_content_percent = train_df.where(train_df.content.str.len() == 0).dropna().groupby(train_df.category).size() / \\\n",
    "train_df.groupby(train_df.category).size() * 100\n",
    "pd.DataFrame(empty_content_percent,columns=['Percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d number of unique words, and number of total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_overall_words = train_df['sentence_length'].sum()\n",
    "print(\"Number of overall words: {}\".format(num_overall_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(set(reduce(set.union , [set(x) for x in train_df['content'].tolist()])).union(reduce(set.union , [set(x) for x in train_df['subject'].tolist()])))\n",
    "\n",
    "print(\"Number of unique words: {}\".format(num_unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = get_categories('categories.txt')\n",
    "label_encoder = get_label_encoder(categories)\n",
    "X_train, y_train, X_test = get_train_test(train, test, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_test_counts = count_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tfidf'] = X_train_tfidf.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tfidf'] = X_test_tfidf.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped_categories_tfidf = train_df.groupby(train_df.category)['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = {}\n",
    "for n,g in train_grouped_categories_tfidf:\n",
    "    class_label = label_encoder.transform([str(n)])[0]\n",
    "    tfidf_mat = np.array(g.values)\n",
    "    tfidf_mat = np.stack(tfidf_mat, axis=0)\n",
    "    centroids[class_label] = tfidf_mat.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_centroid(x):\n",
    "    min_d = np.inf\n",
    "    min_cat = -1\n",
    "    for k,v in centroids.iteritems():\n",
    "        d = euclidean(x,v)\n",
    "        if d < min_d:\n",
    "            min_d = d\n",
    "            min_cat = k\n",
    "    return min_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['min_centroid'] = train_df['tfidf'].apply(compute_min_centroid)\n",
    "test_df['min_centroid'] = test_df['tfidf'].apply(compute_min_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted_categories = label_encoder.inverse_transform(train_df['min_centroid'].values.tolist())\n",
    "test_predicted_categories = label_encoder.inverse_transform(test_df['min_centroid'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_vs_truth = pd.DataFrame(np.vstack((train_df['category'].values,train_predicted_categories))).transpose()\n",
    "train_pred_vs_truth.columns = ['truth','pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (len(train_pred_vs_truth.where(train_pred_vs_truth['truth'] == train_pred_vs_truth['pred']).dropna()) / \\\n",
    "            float(len(train_pred_vs_truth))) * 100\n",
    "print(\"Train accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output1.txt','wb') as f:\n",
    "    for pred in test_predicted_categories:\n",
    "        f.write(\"{}\\n\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from numpy import argmax\n",
    "from collections import Counter\n",
    "\n",
    "hyper_params = {\n",
    "    'validation_split': 0.01,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 3,\n",
    "    'embedding_size': 1024,\n",
    "    'keep_probability': 0.5,\n",
    "    'lstm_size': 32,\n",
    "    'max_sequence': 100,\n",
    "    'l2_regularization': 0.01\n",
    "}\n",
    "\n",
    "categories = get_categories('categories.txt')\n",
    "label_encoder = get_label_encoder(categories)\n",
    "X_train, y_train, X_test = get_train_test(train, test, label_encoder)\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(set(X_train).union(set(X_test)))\n",
    "train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
    "test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(train_tokenized, maxlen=hyper_params['max_sequence'])\n",
    "X_test = sequence.pad_sequences(test_tokenized, maxlen=hyper_params['max_sequence'])\n",
    "vocab_size = len(Counter([token for sublist in train_tokenized + test_tokenized for token in sublist])) + 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, hyper_params['embedding_size']))\n",
    "model.add(Bidirectional(LSTM(hyper_params['lstm_size'])))\n",
    "model.add(Dropout(1 - hyper_params['keep_probability']))\n",
    "model.add(Dense(len(categories), activation='softmax'))          \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=hyper_params['batch_size'], epochs=hyper_params['epochs'], \n",
    "          validation_split=hyper_params['validation_split'], shuffle=True, \n",
    "          callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=1)])\n",
    "\n",
    "predictions = [label_encoder.inverse_transform([argmax(result)])[0] for result in model.predict(X_test)]\n",
    "output_file = open('output2.txt', 'w')\n",
    "for item in predictions:\n",
    "    output_file.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
