{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from helper import get_train_test, lowercase, tokenize, remove_punctuation, remove_stopwords, \\\n",
    "    remove_non_letters, stemming, correct_spelling, reduce_lengthening, subsampling, save_preprocessed_data, \\\n",
    "    get_categories, get_train_test_data, get_label_encoder\n",
    "    \n",
    "config = {\n",
    "    'lowercase': True,\n",
    "    'stemming': True,\n",
    "    'remove_stopwords': True,\n",
    "    'remove_non_letters': True,\n",
    "    'remove_punctuation': True,\n",
    "    'correct_spelling': True,\n",
    "    'reduce_lengthening': True,\n",
    "    'subsampling': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_train_test_data('train_data.txt', 'test_data.txt')\n",
    "\n",
    "# subsampling\n",
    "if config['subsampling']:\n",
    "    subsampling(train)\n",
    "    subsampling(test)\n",
    "\n",
    "# lowercasing\n",
    "if config['lowercase']:\n",
    "    lowercase(train)\n",
    "    lowercase(test)\n",
    "\n",
    "# removing non letters\n",
    "if config['remove_non_letters']:\n",
    "    remove_non_letters(train)\n",
    "    remove_non_letters(test)\n",
    "\n",
    "# removing punctuation\n",
    "if config['remove_punctuation']:\n",
    "    remove_punctuation(train)\n",
    "    remove_punctuation(test)\n",
    "\n",
    "# tokenizing\n",
    "tokenize(train)\n",
    "tokenize(test)\n",
    "\n",
    "# reducing length\n",
    "if config['reduce_lengthening']:\n",
    "    reduce_lengthening(train)\n",
    "    reduce_lengthening(test)\n",
    "    \n",
    "# correcting spelling\n",
    "if config['correct_spelling']:\n",
    "    correct_spelling(train)\n",
    "    correct_spelling(test)\n",
    "\n",
    "# removing stopwords\n",
    "if config['remove_stopwords']:\n",
    "    remove_stopwords(train)\n",
    "    remove_stopwords(test)\n",
    "    \n",
    "# stemming\n",
    "if config['stemming']:\n",
    "    stemming(train)\n",
    "    stemming(test)\n",
    "    \n",
    "# saving preprocessed data\n",
    "save_preprocessed_data(train, 'train_data.pkl')\n",
    "save_preprocessed_data(test, 'test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_preprocessed_data\n",
    "\n",
    "train, test = load_preprocessed_data('train_data.pkl', 'test_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "18000/18000 [==============================] - 102s 6ms/step - loss: 2.5478 - acc: 0.2429 - val_loss: 2.1357 - val_acc: 0.3645\n",
      "Epoch 2/10\n",
      "18000/18000 [==============================] - 94s 5ms/step - loss: 1.7220 - acc: 0.4943 - val_loss: 1.7513 - val_acc: 0.5040\n",
      "Epoch 3/10\n",
      "18000/18000 [==============================] - 95s 5ms/step - loss: 1.2172 - acc: 0.6469 - val_loss: 1.7883 - val_acc: 0.5015\n",
      "Epoch 4/10\n",
      "18000/18000 [==============================] - 97s 5ms/step - loss: 0.8858 - acc: 0.7452 - val_loss: 1.9359 - val_acc: 0.5090\n",
      "Epoch 5/10\n",
      "18000/18000 [==============================] - 93s 5ms/step - loss: 0.6626 - acc: 0.8081 - val_loss: 2.0290 - val_acc: 0.5245\n",
      "Epoch 6/10\n",
      "18000/18000 [==============================] - 90s 5ms/step - loss: 0.5022 - acc: 0.8596 - val_loss: 2.1995 - val_acc: 0.5250\n",
      "Epoch 7/10\n",
      "18000/18000 [==============================] - 109s 6ms/step - loss: 0.3951 - acc: 0.8892 - val_loss: 2.4529 - val_acc: 0.5145\n",
      "Epoch 8/10\n",
      "18000/18000 [==============================] - 72s 4ms/step - loss: 0.3041 - acc: 0.9148 - val_loss: 2.6782 - val_acc: 0.5225\n",
      "Epoch 9/10\n",
      "18000/18000 [==============================] - 66s 4ms/step - loss: 0.2615 - acc: 0.9258 - val_loss: 2.8171 - val_acc: 0.5175\n",
      "Epoch 10/10\n",
      "18000/18000 [==============================] - 63s 3ms/step - loss: 0.2201 - acc: 0.9384 - val_loss: 2.9026 - val_acc: 0.5230\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from numpy import argmax\n",
    "\n",
    "hyper_params = {\n",
    "    'validation_split': 0.1,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10,\n",
    "    'embedding_size': 128,\n",
    "    'keep_probability': 0.9,\n",
    "    'lstm_size': 50,\n",
    "    'dense_size': 50,\n",
    "    'max_sequence': 100\n",
    "}\n",
    "\n",
    "categories = get_categories('categories.txt')\n",
    "label_encoder = get_label_encoder(categories)\n",
    "X_train, y_train, X_test = get_train_test(train, test, label_encoder)\n",
    "vocabulary = set(X_train).union(set(X_test))\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(vocabulary)\n",
    "train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
    "test_tokenized = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(train_tokenized, maxlen=hyper_params['max_sequence'])\n",
    "X_test = sequence.pad_sequences(test_tokenized, maxlen=hyper_params['max_sequence'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary) + 1, hyper_params['embedding_size']))\n",
    "model.add(LSTM(hyper_params['lstm_size']))\n",
    "model.add(Dropout(1 - hyper_params['keep_probability']))\n",
    "model.add(Dense(hyper_params['dense_size'], activation='relu'))\n",
    "model.add(Dropout(1 - hyper_params['keep_probability']))\n",
    "model.add(Dense(len(categories), activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=hyper_params['batch_size'], epochs=hyper_params['epochs'], \n",
    "                    validation_split=hyper_params['validation_split'], shuffle=True)\n",
    "\n",
    "predictions = [label_encoder.inverse_transform([argmax(result)])[0] for result in model.predict(X_test)]\n",
    "output_file = open('output2.txt', 'w')\n",
    "for item in predictions:\n",
    "    output_file.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
